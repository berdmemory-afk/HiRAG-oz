# Configuration for Context Manager

[embedding]
api_url = "https://chutes-intfloat-multilingual-e5-large.chutes.ai/v1/embeddings"
api_token = "cpk_37140d33ae1f4a77ba9980e4fc78a624.25e244203d585ca49b14a4bee55bfda2.MFjdI47zPJZVD16144TNJWv8xlJxBRil"
batch_size = 32
timeout_secs = 30
max_retries = 3
cache_enabled = true
cache_ttl_secs = 3600
cache_size = 1000
tls_enabled = true
tls_verify = true

[vector_db]
url = "http://localhost:6334"
# api_key = "optional_api_key"
collection_prefix = "contexts"
vector_size = 1024
distance = "Cosine"
timeout_secs = 10
tls_enabled = false
tls_verify = true

[hirag]
l1_size = 10
l2_size = 100
l3_enabled = true
max_context_tokens = 4000
relevance_threshold = 0.7
token_estimator = { type = "CharacterBased", chars_per_token = 4.0 }
retrieval_strategy = { l1_allocation = 0.3, l2_allocation = 0.4, l3_allocation = 0.3, min_contexts_per_level = 1 }
ranking_weights = { similarity_weight = 0.5, recency_weight = 0.2, level_weight = 0.2, frequency_weight = 0.1 }
gc_enabled = false
gc_interval_secs = 300
l2_ttl_secs = 3600
l3_ttl_secs = 86400

[protocol]
version = "1.0.0"
codec = "json"
max_message_size_mb = 10

[logging]
level = "info"
format = "json"

[server]
port = 8081
host = "0.0.0.0"
max_body_size_mb = 10
[token_budget]
# Token budget configuration for â‰¤8k token enforcement
# Based on brainstorming.md specifications
system_tokens = 700          # System/Instructions: 600-800 tokens
running_brief = 1200         # Running Brief: 1,000-1,500 tokens
recent_turns = 450           # Recent Turns: 300-600 tokens
retrieved_context = 3750     # Retrieved Context: 3,000-4,500 tokens (8-12 snippets)
completion = 1000            # Completion: 800-1,200 tokens
max_total = 8000             # Maximum total tokens per turn

[vision]
# Vision API configuration for DeepSeek OCR integration
enabled = true                          # Enable/disable OCR integration globally
service_url = "http://localhost:8080"  # DeepSeek service endpoint
api_key = ""                            # API key (or set VISION_API_KEY env var)
timeout_ms = 5000                       # Request timeout in milliseconds
max_regions_per_request = 16            # Maximum regions per decode request
default_fidelity = "10x"                # Default fidelity level (20x, 10x, 5x, 1x)
decode_cache_ttl_secs = 600             # Cache TTL in seconds (10 minutes)
decode_cache_max_size = 1000            # Maximum cache entries
max_concurrent_decodes = 16             # Maximum concurrent decode requests
retry_attempts = 2                      # Number of retry attempts
retry_backoff_ms = 200                  # Base backoff in milliseconds
circuit_breaker_failures = 5            # Circuit breaker failure threshold
circuit_breaker_reset_secs = 30         # Circuit breaker reset timeout
log_redact_text = true                  # Redact OCR text from logs

[facts]
# Facts store configuration for neuro-symbolic reasoning
collection_name = "facts"               # Qdrant collection name for facts
dedup_enabled = true                    # Enable hash-based deduplication
confidence_threshold = 0.8              # Minimum confidence for fact insertion
max_facts_per_query = 100               # Maximum facts returned per query

[token_estimator]
# Token estimation strategy configuration
# Options: "tiktoken" (production, accurate) or "word_based" (fallback)
strategy = "tiktoken"                   # Use tiktoken for accurate token counting
tokens_per_word = 1.3                   # Fallback ratio for word-based estimation

[summarizer]
# LLM summarizer configuration for running brief compression
endpoint = "http://localhost:8080/v1/chat/completions"  # OpenAI-compatible endpoint
api_key = ""                            # Optional API key (leave empty if not needed)
model = "gpt-3.5-turbo"                 # Model to use for summarization
timeout_secs = 30                       # Request timeout in seconds
max_retries = 3                         # Maximum retry attempts on failure
